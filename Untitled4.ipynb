{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f59474-6ee2-4f76-aa3c-380887ecdcda",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed887cf9-aff1-46f8-9b18-973c20a2a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8bec3c2-1613-434f-bca8-7f93b0cdd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_df = pd.read_json('train.json')\n",
    "val_df = pd.read_json('val.json')\n",
    "test_df = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9692d-d08a-4338-9976-9f98c939d3bb",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf19218a-505c-4ba7-bd41-97975be2848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatenate references\n",
    "def concatenate_references(row):\n",
    "    references = []\n",
    "    num_refs = int(row.get('num_references', 0))  # Avoid KeyError\n",
    "    for i in range(num_refs):\n",
    "        ref_key = f'ref_abstract.cite_{i}.abstract'\n",
    "        if ref_key in row:  # Ensure key exists\n",
    "            references.append(row.get(ref_key, ''))  # Avoid KeyError\n",
    "    return ' '.join(references).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48fb1632-57ea-45be-97f7-64cc76bad51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to create a new column with concatenated references\n",
    "train_df['references'] = train_df.apply(concatenate_references, axis=1)\n",
    "val_df['references'] = val_df.apply(concatenate_references, axis=1)\n",
    "test_df['references'] = test_df.apply(concatenate_references, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332cbfc-1579-4856-a2df-4df750f45ba1",
   "metadata": {},
   "source": [
    "# Bert Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e9959b3-fc12-4822-a0e5-202ce7906fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a29a7da8e634d7c88005850a257f968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atoma\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\atoma\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bef50df4cf450ab11053dabfaedc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953c2604e2f24f85a9f8f62ee97a352a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07986b6424904281ae66c0b56210ea17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ReferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        \n",
    "        # Tokenize abstract and references\n",
    "        abstract_tokens = tokenize_function(row['abstract'])\n",
    "        references_tokens = tokenize_function(row['references'])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": abstract_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": abstract_tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": references_tokens[\"input_ids\"].squeeze(0)  # Treat references as labels\n",
    "        }\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ReferenceDataset(train_df)\n",
    "val_dataset = ReferenceDataset(val_df)\n",
    "test_dataset = ReferenceDataset(test_df)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc3ae9-5381-4d22-a029-8f0f1f6427c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
